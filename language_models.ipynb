{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "A5ykFe3GXJ9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77a739b8-6279-4440-865f-282ba104f308"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Structure of data:  DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input', 'output', 'instruction'],\n",
            "        num_rows: 33955\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "!pip install -q datasets\n",
        "!pip install -q gensim\n",
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\")\n",
        "print('Structure of data: ', ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is a dictionary ```train``` as the key and another dictionary ```Dataset```  as the value, the keys of this dictinoary are ```features``` and ```num_rows```, features being one of the ```input```, ```output```, or ```instruction```.\n",
        "\n",
        "The ```num_rows``` suggests that there are 33955 questions and answers with instructions respectively.\n",
        "\n",
        "Let us have a look at how each one of these are:"
      ],
      "metadata": {
        "id": "qSUwwtanf6O8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Input :', ds['train']['input'][0])\n",
        "print(f'Output :', ds['train']['output'][0])\n",
        "print(f'Instruction :', ds['train']['instruction'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foiD3SR3gRd8",
        "outputId": "004092cb-f0dc-4d60-f66a-36e1051fce64"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : What is the relationship between very low Mg2+ levels, PTH levels, and Ca2+ levels?\n",
            "Output : Very low Mg2+ levels correspond to low PTH levels which in turn results in low Ca2+ levels.\n",
            "Instruction : Answer this question truthfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ```input```'s are questions,\n",
        "\n",
        "The ```output```'s are answers and,\n",
        "\n",
        "The ```instruction```'s are the instructions to answer the questions."
      ],
      "metadata": {
        "id": "MaBPHcJkh6cU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lowercasing the text, tokenizing based on spaces to prepare the text for training."
      ],
      "metadata": {
        "id": "ko2YYxtyjdho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "\n",
        "tokenized_sentences = [re.sub('\\W', ' ', sentence).lower().split() for sentence in ds['train']['output']]\n",
        "#\\W removes the non word characters thereby removing the '+','/','-' that could be good to keep\n",
        "for i in tokenized_sentences[:5]:\n",
        "  print(i)\n",
        "\n",
        "print(f'Minimum sentence len: ', min([len(i)for i in tokenized_sentences]))\n",
        "print(f'Maximum sentence len: ', max([len(i)for i in tokenized_sentences]))\n",
        "print(f'Average sentence len: ', np.mean([len(i)for i in tokenized_sentences]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rETmU3C1iQap",
        "outputId": "03db5043-94c6-4ba8-8003-1928f1fe313f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['very', 'low', 'mg2', 'levels', 'correspond', 'to', 'low', 'pth', 'levels', 'which', 'in', 'turn', 'results', 'in', 'low', 'ca2', 'levels']\n",
            "['low', 'estradiol', 'production', 'leads', 'to', 'genitourinary', 'syndrome', 'of', 'menopause', 'atrophic', 'vaginitis']\n",
            "['low', 'rem', 'sleep', 'latency', 'and', 'experiencing', 'hallucinations', 'sleep', 'paralysis', 'suggests', 'narcolepsy']\n",
            "['pth', 'independent', 'hypercalcemia', 'which', 'can', 'be', 'caused', 'by', 'cancer', 'granulomatous', 'disease', 'or', 'vitamin', 'd', 'intoxication']\n",
            "['the', 'level', 'of', 'anti', 'm√ºllerian', 'hormone', 'is', 'directly', 'related', 'to', 'ovarian', 'reserve', 'a', 'lower', 'level', 'indicates', 'a', 'lower', 'ovarian', 'reserve']\n",
            "Minimum sentence len:  0\n",
            "Maximum sentence len:  247\n",
            "Average sentence len:  54.24835812104256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now want to train the Word2Vec on the ```outputs```."
      ],
      "metadata": {
        "id": "e0Az8VgTnsdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "model = Word2Vec(tokenized_sentences, vector_size=100, min_count=2, window=10)\n",
        "print(f'Learnt vectors: ',len(model.wv))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieXfOgnFn2Tr",
        "outputId": "de32d6dd-730e-4b36-bba5-7ff83cfae066"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learnt vectors:  17679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some things to try,\n",
        "\n",
        "try different terms\n",
        "\n",
        "most similar words"
      ],
      "metadata": {
        "id": "12r2ck8T0yPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# term = 'sickness'\n",
        "# term = 'fever'\n",
        "# term = 'cure'\n",
        "term = 'drugs'\n",
        "\n",
        "sims = model.wv.most_similar(term, topn=5)\n",
        "sims"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjMOrfOA1qBA",
        "outputId": "eec74640-5f17-45d6-e2ac-25bd60b2b9fe"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('agents', 0.8524566888809204),\n",
              " ('inhibitors', 0.7399193644523621),\n",
              " ('medications', 0.7008272409439087),\n",
              " ('drug', 0.6907287240028381),\n",
              " ('chemicals', 0.6843883395195007)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q hnswlib\n",
        "!pip install -q sentence-transformers\n",
        "import hnswlib\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "corpus = ds['train']['output'][:100]    # create a corpus of 100 docs\n",
        "model = SentenceTransformer('all-mpnet-base-v2')\n",
        "corpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n",
        "\n",
        "\n",
        "index = hnswlib.Index(space='cosine', dim=corpus_embeddings.size(1))"
      ],
      "metadata": {
        "id": "P9Vd9Rvm_XKN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "index_path = './hnswlib.index'\n",
        "\n",
        "if not os.path.exists(index_path):\n",
        "  print(\"Creating a HNSWLIB index\")\n",
        "  corpus = ds['train']['output']\n",
        "  model = SentenceTransformer('all-mpnet-base-v2')\n",
        "  corpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n",
        "  index = hnswlib.Index(space='cosine', dim=corpus_embeddings.size(1))\n",
        "  index.init_index(max_elements = corpus_embeddings.size(0), ef_construction=128, M=64)\n",
        "  index.add_items(corpus_embeddings.cpu(), list(range(len(corpus_embeddings))))\n",
        "  print(\"Saving index to:\", index_path)\n",
        "  index.save_index(index_path)\n",
        "else:\n",
        "  index.load_index(index_path)\n",
        ""
      ],
      "metadata": {
        "id": "3DnbU3OpBLgn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = ds['train']['input'][0]\n",
        "print(f'Query : ', query)\n",
        "query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "ids, distances = index.knn_query(query_embedding.cpu(), k=16)\n",
        "out = ds['train']['output'][ids[0][np.argmin(distances[0])]]\n",
        "out"
      ],
      "metadata": {
        "id": "1tnJK1g6B0OX",
        "outputId": "761caff0-8aa6-484c-f940-1b7664c45982",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query :  What is the relationship between very low Mg2+ levels, PTH levels, and Ca2+ levels?\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Very low Mg2+ levels correspond to low PTH levels which in turn results in low Ca2+ levels.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Re-ranking"
      ],
      "metadata": {
        "id": "ftujIPt1YKPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "xenc_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "model_inputs = [(query, corpus[i]) for i in ids[0]]\n",
        "cross_scores = xenc_model.predict(model_inputs)\n",
        "print(\"Cross-encoder model re-ranking results\")\n",
        "print(f\"Query: \\\"{query} \\\"\")\n",
        "print(\"-------------------------\")\n",
        "for idx in np.argsort(-cross_scores)[:3]:\n",
        "  print(f\"Score: {cross_scores[idx]:.4f}\\nDocument: \\\" {corpus[ids[0][idx]]}\\\"\")"
      ],
      "metadata": {
        "id": "6FPTw1phDPkX",
        "outputId": "bd51c56a-4cc3-413d-95c5-eac63164d550",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-encoder model re-ranking results\n",
            "Query: \"What is the relationship between very low Mg2+ levels, PTH levels, and Ca2+ levels? \"\n",
            "-------------------------\n",
            "Score: 8.8953\n",
            "Document: \" Very low Mg2+ levels correspond to low PTH levels which in turn results in low Ca2+ levels.\"\n",
            "Score: 4.0440\n",
            "Document: \" Low Ca2+ and low PTH is seen in primary hypoparathyroidism.\"\n",
            "Score: -4.2314\n",
            "Document: \" PTH-independent hypercalcemia, which can be caused by cancer, granulomatous disease, or vitamin D intoxication.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the first few of the documents as the context and prompt an LLM to generate text"
      ],
      "metadata": {
        "id": "gHl2rI67fDCp"
      }
    }
  ]
}